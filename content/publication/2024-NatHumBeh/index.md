---
title: "Testing Theory of Mind in Large Language Models and Humans"

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here 
# and it will be replaced with their full name and linked to their profile.
authors:
- Strachan JWA
- Albergo D
- Borghini G
- Pansardi O
- Scaliti E 
- Gupta S
- Saxena K
- Rufo A
- Panzeri S
- Manzi G
- Graziano MSA
- Becchio C

date: "2024-01-01T00:00:00Z" # CHANGE
doi: "10.1177/17456916231182923"

# Schedule page publish date (NOT publication's date).
publishDate: "2023-07-17T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["3"]

# Publication name and optional abbreviated publication name.
publication: Under revision at Nature Human Behaviour
publication_short: under revision at Nature Human Behaviour

abstract: At the core of what defines us as humans is the concept of Theory of Mind; the ability to track other people’s mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models might have developed an artificial Theory of Mind. Here, we present results from a comprehensive battery of measurements spanning different Theory of Mind abilities, from understanding false beliefs, to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a large sample of human participants.  Across the battery of Theory of Mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs, and misdirection, but struggled with detecting faux pas. Faux pas was also the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that LLaMA2’s superiority was illusory, possibly reflecting a bias towards attributing ignorance. In contrast, GPT’s poor performance originated from a hyper conservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans, but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.

# Summary. An optional shortened abstract.

tags: 
- social cognition
- theory of mind
- artificial intelligence
- large language models

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
url_source: ""
#url_pdf: "https://www.researchsquare.com/article/rs-3262385/v1.pdf"

links:
- name: Preprint
  url: https://doi.org/10.21203/rs.3.rs-3262385/

---


